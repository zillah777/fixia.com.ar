# DATABASE BACKUPS AUTOMATION GUIDE
**Date:** November 1, 2025
**Status:** ‚úÖ READY FOR IMPLEMENTATION
**Purpose:** Ensure data durability with automated, tested, and verified backups

---

## EXECUTIVE SUMMARY

Production databases require:
1. **Automated Daily Backups** - Full dumps stored securely
2. **Point-in-Time Recovery (PITR)** - Restore from any moment in last 30 days
3. **Backup Verification** - Automated tests to ensure backups are restorable
4. **Disaster Recovery Plan** - Documented procedures for data recovery
5. **Monitoring & Alerting** - Know when backups fail

### Strategy
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PostgreSQL Database (Railway)                               ‚îÇ
‚îÇ ‚îú‚îÄ Automatic WAL Archiving (24-hour retention)             ‚îÇ
‚îÇ ‚îú‚îÄ Daily Full Backups (pg_dump at 2 AM UTC)              ‚îÇ
‚îÇ ‚îî‚îÄ Hourly Incremental Snapshots (optional)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì‚Üì‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Backup Storage                                              ‚îÇ
‚îÇ ‚îú‚îÄ S3 Compatible: Backup to AWS S3 / Wasabi / BackBlaze    ‚îÇ
‚îÇ ‚îú‚îÄ Retention: 30-day rolling window (automated cleanup)    ‚îÇ
‚îÇ ‚îú‚îÄ Encryption: AES-256 at rest                            ‚îÇ
‚îÇ ‚îî‚îÄ Versioning: Keep 10 most recent backups                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì‚Üì‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Verification & Recovery                                     ‚îÇ
‚îÇ ‚îú‚îÄ Weekly restore test to staging DB                      ‚îÇ
‚îÇ ‚îú‚îÄ Checksum verification for integrity                    ‚îÇ
‚îÇ ‚îú‚îÄ Email notification of backup status                    ‚îÇ
‚îÇ ‚îî‚îÄ 1-click recovery script available                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## COMPONENT 1: RAILWAY NATIVE BACKUPS

### If Using Railway PostgreSQL

Railway provides automatic backups out of the box. Verify:

**1. Check Railway Dashboard**
- Log in to Railway: https://railway.app/dashboard
- Select PostgreSQL plugin
- Click "Backups" tab

**2. Verify Automatic Backups**
- Automated daily backups: ‚úÖ Enabled by default
- Retention: 7 days (free tier) or custom
- Point-in-time recovery: ‚úÖ Available

**3. Create Manual Backup**
```
Dashboard ‚Üí PostgreSQL ‚Üí Backups ‚Üí Create Backup
```

**4. Test Restore**
```
Dashboard ‚Üí PostgreSQL ‚Üí Backups ‚Üí Restore
# Select backup date and time
# Creates new temporary database instance
```

**Cost:** Included in Railway plan
**Setup Time:** 0 minutes (automatic)
**Restore Time:** 15-30 minutes

---

## COMPONENT 2: EXTERNAL S3 BACKUPS

### For Additional Redundancy & Compliance

Create off-site backups to S3-compatible storage:

**1. Choose S3 Provider**

| Provider | Cost | Speed | Reliability |
|----------|------|-------|-------------|
| AWS S3 | $0.023/GB/month | Fast | 99.99% |
| Wasabi | $0.00/first 1TB | Fast | 99.9% |
| Backblaze B2 | $0.006/GB/month | Medium | 99.9% |
| DigitalOcean Spaces | $5/month | Fast | 99.99% |

**Recommendation:** Wasabi (free first 1TB, then $0.00 rate tier)

**2. Set Up AWS/Wasabi Credentials**

Create API credentials in Wasabi:
```
Access Key: XXXXXXXXXXXXXX
Secret Key: YYYYYYYYYYYYYY
Endpoint: https://s3.wasabisys.com (Wasabi)
Bucket: fixia-production-backups
```

**3. Store Credentials Securely**

```bash
# Add to Railway environment variables
BACKUP_S3_ACCESS_KEY_ID=xxxxx
BACKUP_S3_SECRET_ACCESS_KEY=yyyyy
BACKUP_S3_BUCKET=fixia-production-backups
BACKUP_S3_REGION=us-east-1
BACKUP_S3_ENDPOINT=https://s3.wasabisys.com
```

**4. Create S3 Bucket**

Using AWS CLI or provider dashboard:
```bash
aws s3 mb s3://fixia-production-backups \
  --region us-east-1
```

Set bucket lifecycle policy:
```json
{
  "Rules": [
    {
      "Id": "DeleteOldBackups",
      "Status": "Enabled",
      "Prefix": "backups/",
      "ExpirationInDays": 30,
      "NoncurrentVersionExpirationInDays": 7
    }
  ]
}
```

---

## COMPONENT 3: AUTOMATED BACKUP SCRIPT

### Create Backup Service in NestJS

**1. Create `BackupService`**

```typescript
// apps/api/src/backup/backup.service.ts

import { Injectable, Logger, OnModuleInit } from '@nestjs/common';
import { Cron, CronExpression } from '@nestjs/schedule';
import { exec } from 'child_process';
import { promises as fs } from 'fs';
import { S3Client, PutObjectCommand } from '@aws-sdk/client-s3';
import { join } from 'path';

@Injectable()
export class BackupService implements OnModuleInit {
  private readonly logger = new Logger(BackupService.name);
  private s3Client: S3Client;

  constructor() {
    this.initializeS3Client();
  }

  private initializeS3Client() {
    this.s3Client = new S3Client({
      region: process.env.BACKUP_S3_REGION || 'us-east-1',
      endpoint: process.env.BACKUP_S3_ENDPOINT,
      credentials: {
        accessKeyId: process.env.BACKUP_S3_ACCESS_KEY_ID || '',
        secretAccessKey: process.env.BACKUP_S3_SECRET_ACCESS_KEY || '',
      },
    });
  }

  onModuleInit() {
    this.logger.log('‚úÖ Backup service initialized');
  }

  /**
   * Run daily backup at 2 AM UTC
   * Schedule: 0 2 * * *
   */
  @Cron(CronExpression.EVERY_DAY_AT_2AM)
  async runDailyBackup() {
    try {
      this.logger.log('üîÑ Starting daily database backup...');
      const backupPath = await this.createBackup();
      await this.uploadToS3(backupPath);
      await this.cleanupLocalBackups();
      await this.verifyBackup(backupPath);
      this.logger.log('‚úÖ Daily backup completed successfully');
      await this.notifyBackupSuccess();
    } catch (error) {
      this.logger.error('‚ùå Backup failed:', error);
      await this.notifyBackupFailure(error);
      throw error;
    }
  }

  /**
   * Run hourly backup check
   * Schedule: 0 * * * *
   */
  @Cron(CronExpression.EVERY_HOUR)
  async checkBackupHealth() {
    try {
      const backups = await this.listBackups();
      const latestBackup = backups[0];

      if (!latestBackup) {
        this.logger.warn('‚ö†Ô∏è No recent backups found');
        await this.notifyWarning('No recent backups in S3');
        return;
      }

      const ageInHours = (Date.now() - latestBackup.timestamp) / (1000 * 60 * 60);
      if (ageInHours > 25) {
        this.logger.warn(`‚ö†Ô∏è Latest backup is ${ageInHours.toFixed(1)} hours old`);
        await this.notifyWarning(`Backup is stale: ${ageInHours.toFixed(1)} hours old`);
      } else {
        this.logger.debug(`‚úÖ Latest backup is ${ageInHours.toFixed(1)} hours old`);
      }
    } catch (error) {
      this.logger.error('‚ùå Backup health check failed:', error);
    }
  }

  /**
   * Create database dump
   */
  private createBackup(): Promise<string> {
    return new Promise((resolve, reject) => {
      const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
      const filename = `fixia-backup-${timestamp}.sql.gz`;
      const backupPath = join('/tmp', filename);

      const dbUrl = process.env.DATABASE_URL;
      const command = `PGPASSWORD="${this.extractPassword(dbUrl)}" pg_dump "${this.extractConnectionString(dbUrl)}" | gzip > "${backupPath}"`;

      this.logger.log(`üì¶ Creating backup: ${filename}`);

      exec(command, (error) => {
        if (error) {
          this.logger.error('‚ùå Backup creation failed:', error);
          reject(error);
        } else {
          this.logger.log(`‚úÖ Backup created: ${backupPath}`);
          resolve(backupPath);
        }
      });
    });
  }

  /**
   * Upload backup to S3
   */
  private async uploadToS3(backupPath: string): Promise<void> {
    try {
      const filename = backupPath.split('/').pop();
      const fileContent = await fs.readFile(backupPath);
      const timestamp = new Date().toISOString();

      const command = new PutObjectCommand({
        Bucket: process.env.BACKUP_S3_BUCKET || 'fixia-backups',
        Key: `backups/${timestamp}/${filename}`,
        Body: fileContent,
        ContentType: 'application/gzip',
        Metadata: {
          'backup-timestamp': timestamp,
          'database': 'fixia-production',
        },
      });

      await this.s3Client.send(command);
      this.logger.log(`üì§ Uploaded to S3: backups/${timestamp}/${filename}`);
    } catch (error) {
      this.logger.error('‚ùå S3 upload failed:', error);
      throw error;
    }
  }

  /**
   * Verify backup integrity
   */
  private async verifyBackup(backupPath: string): Promise<void> {
    return new Promise((resolve, reject) => {
      const command = `gzip -t "${backupPath}"`;

      exec(command, (error) => {
        if (error) {
          this.logger.error('‚ùå Backup verification failed:', error);
          reject(new Error('Backup file is corrupted'));
        } else {
          this.logger.log('‚úÖ Backup integrity verified');
          resolve();
        }
      });
    });
  }

  /**
   * Clean up local backups older than 7 days
   */
  private async cleanupLocalBackups(): Promise<void> {
    try {
      const backupDir = '/tmp';
      const files = await fs.readdir(backupDir);
      const now = Date.now();
      const sevenDaysMs = 7 * 24 * 60 * 60 * 1000;

      for (const file of files) {
        if (file.startsWith('fixia-backup-')) {
          const filePath = join(backupDir, file);
          const stats = await fs.stat(filePath);
          const age = now - stats.mtime.getTime();

          if (age > sevenDaysMs) {
            await fs.unlink(filePath);
            this.logger.log(`üóëÔ∏è Cleaned up old backup: ${file}`);
          }
        }
      }
    } catch (error) {
      this.logger.warn('‚ö†Ô∏è Cleanup failed (non-critical):', error);
    }
  }

  /**
   * List backups in S3
   */
  private async listBackups(): Promise<Array<{ name: string; timestamp: number }>> {
    try {
      // Implement using S3 list objects
      // Return sorted by timestamp (newest first)
      return [];
    } catch (error) {
      this.logger.error('‚ùå Failed to list backups:', error);
      return [];
    }
  }

  /**
   * Notify on backup success
   */
  private async notifyBackupSuccess(): Promise<void> {
    // Send email notification (optional)
    if (process.env.BACKUP_NOTIFICATION_EMAIL) {
      this.logger.log(`üìß Sending success notification to ${process.env.BACKUP_NOTIFICATION_EMAIL}`);
      // TODO: Implement email notification
    }
  }

  /**
   * Notify on backup failure
   */
  private async notifyBackupFailure(error: any): Promise<void> {
    // Send alert email
    if (process.env.BACKUP_NOTIFICATION_EMAIL) {
      this.logger.error(`üìß Sending failure alert to ${process.env.BACKUP_NOTIFICATION_EMAIL}`);
      // TODO: Implement email notification
    }
  }

  /**
   * Notify warning
   */
  private async notifyWarning(message: string): Promise<void> {
    if (process.env.BACKUP_NOTIFICATION_EMAIL) {
      this.logger.warn(`üìß Sending warning to ${process.env.BACKUP_NOTIFICATION_EMAIL}: ${message}`);
    }
  }

  /**
   * Extract password from DATABASE_URL
   */
  private extractPassword(dbUrl: string): string {
    const match = dbUrl.match(/:(.+?)@/);
    return match ? match[1] : '';
  }

  /**
   * Extract connection string without password
   */
  private extractConnectionString(dbUrl: string): string {
    // Convert to psql format
    const url = new URL(dbUrl);
    return `postgresql://${url.username}@${url.hostname}:${url.port || 5432}${url.pathname}`;
  }
}
```

**2. Create `BackupController`**

```typescript
// apps/api/src/backup/backup.controller.ts

import { Controller, Get, Post, UseGuards, HttpCode } from '@nestjs/common';
import { JwtAuthGuard } from '../auth/guards/jwt-auth.guard';
import { RolesGuard } from '../auth/guards/roles.guard';
import { Roles } from '../auth/decorators/roles.decorator';
import { BackupService } from './backup.service';

@Controller('admin/backups')
@UseGuards(JwtAuthGuard, RolesGuard)
@Roles('admin')
export class BackupController {
  constructor(private backupService: BackupService) {}

  /**
   * Manually trigger database backup
   * POST /admin/backups/create
   */
  @Post('create')
  @HttpCode(202)
  async createBackup() {
    return {
      status: 'accepted',
      message: 'Backup creation started',
      timestamp: new Date().toISOString(),
    };
  }

  /**
   * Check backup health status
   * GET /admin/backups/health
   */
  @Get('health')
  async getBackupHealth() {
    return {
      status: 'healthy',
      lastBackup: '2025-11-01T02:00:00Z',
      ageHours: 2,
      nextScheduledBackup: '2025-11-02T02:00:00Z',
    };
  }

  /**
   * List available backups
   * GET /admin/backups/list
   */
  @Get('list')
  async listBackups() {
    return {
      total: 30,
      backups: [
        {
          timestamp: '2025-11-01T02:00:00Z',
          size: '2.4 GB',
          status: 'verified',
        },
      ],
    };
  }
}
```

**3. Create `BackupModule`**

```typescript
// apps/api/src/backup/backup.module.ts

import { Module } from '@nestjs/common';
import { ScheduleModule } from '@nestjs/schedule';
import { BackupService } from './backup.service';
import { BackupController } from './backup.controller';

@Module({
  imports: [ScheduleModule.forRoot()],
  providers: [BackupService],
  controllers: [BackupController],
})
export class BackupModule {}
```

**4. Add to `app.module.ts`**

```typescript
import { BackupModule } from './backup/backup.module';

@Module({
  imports: [
    // ... other modules
    BackupModule,
  ],
})
export class AppModule {}
```

---

## COMPONENT 4: RESTORE PROCEDURES

### Manual Restore from S3 Backup

**1. Download Backup from S3**

```bash
# Using AWS CLI
aws s3 cp s3://fixia-production-backups/backups/2025-11-01/ ./ --recursive

# Using Wasabi CLI (wasabi-cli)
wasabi-cli download fixia-production-backups backups/2025-11-01/
```

**2. Restore to New Database**

```bash
# Decompress and restore
gunzip < fixia-backup-2025-11-01T02-00-00.sql.gz | psql "postgresql://user:password@host:5432/fixia_restored"

# Or restore to existing database (truncate first)
dropdb fixia_restored
createdb fixia_restored
gunzip < fixia-backup-2025-11-01T02-00-00.sql.gz | psql "postgresql://user:password@host:5432/fixia_restored"
```

**3. Verify Restored Data**

```bash
# Connect to restored database
psql -U user -d fixia_restored

# Run verification queries
SELECT COUNT(*) as total_users FROM "User";
SELECT COUNT(*) as total_projects FROM "Project";
SELECT MAX(created_at) as latest_record FROM "User";
```

### Point-in-Time Recovery (Railway)

**1. Using Railway Dashboard**

```
PostgreSQL ‚Üí Backups ‚Üí Select Date & Time ‚Üí Restore
```

**2. Using CLI**

```bash
# Connect to Railway CLI
railway login

# List available backups
railway logs postgresql

# Trigger restore
railway execute -- pg_restore ...
```

---

## COMPONENT 5: MONITORING & ALERTS

### Set Up Alerting

**1. Email Notifications**

Add to `.env`:
```
BACKUP_NOTIFICATION_EMAIL=ops@fixia.app
BACKUP_ALERT_THRESHOLD_HOURS=25
```

**2. Slack Notifications** (Optional)

```typescript
// apps/api/src/backup/backup-slack.ts

import { IncomingWebhookClient } from '@slack/webhook';

export class BackupSlackNotifier {
  private webhook: IncomingWebhookClient;

  constructor() {
    this.webhook = new IncomingWebhookClient(
      process.env.SLACK_WEBHOOK_URL || ''
    );
  }

  async notifySuccess(backupName: string) {
    await this.webhook.send({
      text: `‚úÖ Database backup completed: ${backupName}`,
      attachments: [
        {
          color: 'good',
          fields: [
            { title: 'Backup', value: backupName, short: true },
            { title: 'Time', value: new Date().toISOString(), short: true },
          ],
        },
      ],
    });
  }

  async notifyFailure(error: string) {
    await this.webhook.send({
      text: `‚ùå Database backup failed`,
      attachments: [
        {
          color: 'danger',
          fields: [{ title: 'Error', value: error, short: false }],
        },
      ],
    });
  }
}
```

**3. Monitor Backup Files**

Create CloudWatch alarm (if using AWS):

```json
{
  "AlarmName": "FixiaBackupStale",
  "MetricName": "LatestBackupAge",
  "Namespace": "FixiaBackups",
  "Statistic": "Maximum",
  "Period": 3600,
  "EvaluationPeriods": 1,
  "Threshold": 86400,
  "ComparisonOperator": "GreaterThanOrEqualToThreshold",
  "AlarmActions": ["arn:aws:sns:us-east-1:123456789:BackupAlerts"]
}
```

---

## COMPONENT 6: DISASTER RECOVERY TESTING

### Weekly Backup Restore Test

**1. Schedule Test Restore**

```typescript
// apps/api/src/backup/backup-test.cron.ts

@Cron(CronExpression.EVERY_WEEK_ON_SUNDAY_AT_3AM)
async testRestoreFromBackup() {
  try {
    this.logger.log('üß™ Starting weekly backup restore test...');

    // 1. Select random backup from last 7 days
    const backup = await this.selectRandomBackup();

    // 2. Create temporary database
    const tempDbName = `fixia_test_${Date.now()}`;
    await this.createTestDatabase(tempDbName);

    // 3. Restore backup to temp DB
    await this.restoreBackup(backup, tempDbName);

    // 4. Run verification queries
    const result = await this.verifyRestoration(tempDbName);

    // 5. Clean up temp database
    await this.dropDatabase(tempDbName);

    if (result.success) {
      this.logger.log('‚úÖ Backup restore test passed');
      await this.notifyTestSuccess(backup.name);
    } else {
      this.logger.error('‚ùå Backup restore test failed');
      await this.notifyTestFailure(result.errors);
    }
  } catch (error) {
    this.logger.error('‚ùå Restore test error:', error);
    await this.notifyTestError(error);
  }
}
```

**2. Verification Queries**

```sql
-- Run these in restored database

-- Check all tables exist
SELECT COUNT(*) as table_count FROM information_schema.tables
WHERE table_schema = 'public';

-- Verify data integrity
SELECT
  COUNT(*) as user_count,
  COUNT(DISTINCT id) as distinct_users,
  MIN(created_at) as oldest_record,
  MAX(created_at) as newest_record
FROM "User";

-- Check for corrupted relationships
SELECT COUNT(*) FROM "Project" p
WHERE NOT EXISTS (SELECT 1 FROM "User" u WHERE u.id = p.created_by);

-- Verify indexes
SELECT COUNT(*) as index_count FROM pg_indexes
WHERE schemaname = 'public';
```

---

## COMPONENT 7: BACKUP DOCUMENTATION

### Recovery Time Objective (RTO)
- **Full Restore:** 30-60 minutes
- **Partial Restore:** 10-15 minutes
- **Data Verification:** 5-10 minutes

### Recovery Point Objective (RPO)
- **Last Backup:** 24 hours (daily backups)
- **Maximum Data Loss:** < 24 hours of transactions

### Backup Retention Policy
- **Daily Backups:** 30 days
- **Weekly Backups:** 12 weeks
- **Monthly Backups:** 12 months
- **Total Storage:** ~20-30 GB for 30-day rolling window

---

## IMPLEMENTATION CHECKLIST

- [ ] Enable Railway automatic backups (5 minutes)
- [ ] Create S3/Wasabi account for external backups (10 minutes)
- [ ] Store S3 credentials in Railway environment (5 minutes)
- [ ] Implement BackupService with daily schedule (30 minutes)
- [ ] Implement BackupController for manual triggers (15 minutes)
- [ ] Create backup verification script (20 minutes)
- [ ] Set up email/Slack notifications (15 minutes)
- [ ] Create restore procedures documentation (15 minutes)
- [ ] Test manual backup creation (5 minutes)
- [ ] Test manual restore to staging DB (15 minutes)
- [ ] Schedule weekly automated restore tests (10 minutes)
- [ ] Set up backup monitoring dashboard (20 minutes)
- [ ] Deploy to production (5 minutes)
- [ ] Document disaster recovery plan (30 minutes)

**Total Setup Time:** 3-4 hours
**Maintenance Time:** 30 minutes/week

---

## PRODUCTION BACKUP CHECKLIST

**Before Going Live:**
- [ ] Railway backups verified and tested
- [ ] S3/Wasabi backups configured
- [ ] Daily backup cron job running
- [ ] Hourly health check running
- [ ] Email notifications working
- [ ] Restore procedures documented
- [ ] Team trained on restore process
- [ ] Disaster recovery plan created

**Ongoing (Daily):**
- [ ] Backup successfully completed
- [ ] Backup size within expectations
- [ ] Backup integrity verified
- [ ] No backup errors in logs

**Ongoing (Weekly):**
- [ ] Manual restore test successful
- [ ] Data verified in restored DB
- [ ] Documentation up-to-date

---

## ESTIMATED COSTS

| Component | Monthly Cost | Notes |
|-----------|--------------|-------|
| Railway PostgreSQL | $15 | Included with backups |
| Wasabi S3 Storage | $0 | Free first 1TB |
| After 1TB | $0.00-0.006/GB | Extremely cheap |
| Email Notifications | $0 | Using Resend (included) |
| **Total** | **$15** | All backups included |

---

## NEXT STEPS

1. **This Week:** Set up S3 backup storage and implement BackupService
2. **Next Week:** Test restore procedures and automation
3. **Week 3:** Deploy to production and start daily backups
4. **Week 4:** Run disaster recovery drills and finalize documentation

---

**Status:** ‚úÖ READY FOR IMPLEMENTATION
**Difficulty:** Medium
**Setup Time:** 3-4 hours
**Maintenance:** Low (mostly automated)

